[package]
name = "spike-3-llm-inference"
version = "0.1.0"
edition = "2021"
description = "Validation spike: llama.cpp FFI for local LLM inference via Rust"

[features]
default = ["cuda"]
cuda = ["llama-cpp-2/cuda"]

[dependencies]
llama-cpp-2 = "0.1"
encoding_rs = "0.8"
anyhow = "1"
clap = { version = "4", features = ["derive"] }

[profile.release]
opt-level = 3
lto = "thin"
codegen-units = 1
