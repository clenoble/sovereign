[package]
name = "sovereign-ai"
version = "0.1.0"
edition = "2021"
description = "AI orchestrator with llama.cpp inference (Phase 3)"

[dependencies]
sovereign-core = { workspace = true }
sovereign-db = { workspace = true }
tokio = { workspace = true }
async-trait = { workspace = true }
anyhow = { workspace = true }
tracing = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
chrono = { workspace = true }
llama-cpp-2 = { workspace = true }
encoding_rs = { workspace = true }

# Voice pipeline (optional â€” whisper-rs embeds ggml which conflicts with llama-cpp on Windows)
whisper-rs = { workspace = true, optional = true }
cpal = { workspace = true, optional = true }
ringbuf = { workspace = true, optional = true }

# Wake word (optional due to candle-core version conflicts)
rustpotter = { workspace = true, optional = true }

# Optional P2P deps
sovereign-crypto = { workspace = true, optional = true }
sovereign-p2p = { workspace = true, optional = true }

[dev-dependencies]
sovereign-db = { workspace = true, features = ["test-utils"] }

[features]
default = ["cuda"]
cuda = ["llama-cpp-2/cuda"]
voice-stt = ["whisper-rs", "cpal", "ringbuf"]
wake-word = ["voice-stt", "rustpotter"]
p2p = ["sovereign-crypto", "sovereign-p2p"]
